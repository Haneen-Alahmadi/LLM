{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haneen-Alahmadi/SDAIA-Large-Language-Model/blob/main/Blanks__Arabic_Hate_Speech_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlGpLgIaxp9z"
      },
      "source": [
        "#Hate Speech Detection with AraBERT and HuggingFace\n",
        "In this assignment, we will be exploring the application of the AraBERT model specifically for the task of hate speech detection. We will use the AJGT Sentiment Analysis dataset from K. M. Alomari, H. M. ElSherif, and K. Shaalan, “Arabic tweets sentimental analysis using machine learning,” in Proceedings of the International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, pp. 602–610, Montreal, Canada, June 2017.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JwOAlV5xsGV"
      },
      "source": [
        "# Check which GPU we have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krFwQr32xqLv"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd6vAHTei6PQ"
      },
      "source": [
        "##Installing Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohWiDLEnxJA5"
      },
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!git clone https://github.com/aub-mind/arabert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFxAnGMIx1qG"
      },
      "source": [
        "#Reading Data\n",
        "We will rely on the following libraries for training and evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl-z0SH0gR_C"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/komari6/Arabic-twitter-corpus-AJGT.git"
      ],
      "metadata": {
        "id": "VBYk072elB84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** Read the dataset and arrange the columns name using the set variables:"
      ],
      "metadata": {
        "id": "8T1UM7HcoQkm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKFr-GcJjXEE"
      },
      "source": [
        "DATA_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: Split the data into training and testing (80-20)"
      ],
      "metadata": {
        "id": "K_zISEpsoOjs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFnvKJpojo2y"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWF1oSY_jglL"
      },
      "source": [
        "**Question 3:** Plot the distribution of lengths of sentences in both training and test set. Extract the max_len value to be used later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5seZqZp-cWK7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that the max_len can be chosen to be ___."
      ],
      "metadata": {
        "id": "2Gozk8y2p1os"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULbvRAH24w11"
      },
      "source": [
        "#Training Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by creating the dataset needed for training and testing, we will use the Dataset class from pytorch as our base class.\n",
        "\n",
        "For tokenization, we will be using the autotokenizer from HuggingFace."
      ],
      "metadata": {
        "id": "POaAvq9NtKHl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHazBYgbAugR"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#define the Dataset class\n",
        "class SADataset(Dataset):\n",
        "  def __init__(self, texts, labels, model_name, max_len, label_map):\n",
        "    #hold the text and reviews inside the dataset class\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.label_map = label_map\n",
        "    self.tokenizer_name = model_name\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    #returns the length of the dataset\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    # Select the example based on the item ID\n",
        "    text = str(self.texts[item])\n",
        "    label = self.labels[item]\n",
        "\n",
        "    input_dict = self.tokenizer(\n",
        "          text,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          padding = 'max_length',\n",
        "          truncation= True\n",
        "      )\n",
        "\n",
        "    return InputFeatures(input_ids=input_dict[\"input_ids\"],\n",
        "                         token_type_ids=input_dict['token_type_ids'],\n",
        "                         attention_mask=input_dict[\"attention_mask\"],\n",
        "                         label=self.label_map[self.labels[item]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** Define the evaluation metrics that we will need, including `accuracy_score`, `f1_score`, `precision_score` and `recall_score` from sklearn."
      ],
      "metadata": {
        "id": "5A70UoelxyYO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPOnvGZRlVT"
      },
      "source": [
        "def compute_metrics(pred):\n",
        "  preds = np.argmax(pred.predictions, axis=1)\n",
        "  assert len(preds) == len(pred.label_ids)\n",
        "  \"\"\"\n",
        "  Add metrics calls here\n",
        "  \"\"\"\n",
        "  return {\n",
        "      'macro_f1' : macro_f1,\n",
        "      'macro_f1_pos_neg' : macro_f1_pos_neg,\n",
        "      'macro_precision': macro_precision,\n",
        "      'macro_recall': macro_recall,\n",
        "      'accuracy': acc\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcjtO-0N0u98"
      },
      "source": [
        "## Preprocess the dataset\n",
        "Let's start by defining the AraBERT preprocessor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbMtdGpB0t3w"
      },
      "source": [
        "from arabert.preprocess import ArabertPreprocessor\n",
        "\n",
        "model_name = 'aubmindlab/bert-base-arabertv02'\n",
        "arabert_prep = ArabertPreprocessor(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Apply preprocessing for the text column using the arabert preprocessor:"
      ],
      "metadata": {
        "id": "RWKdP_IN1UpM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-wo9pTC0zXT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract the label to id map:"
      ],
      "metadata": {
        "id": "GgfXN43X1gWV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDikUkPn2DqE"
      },
      "source": [
        "label_list = list(data_test[LABEL_COLUMN].unique())\n",
        "label_map = { v:index for index, v in enumerate(label_list) }\n",
        "print(label_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Create the train and test SADataset:"
      ],
      "metadata": {
        "id": "1JfEWxXM1nDj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2zr1L_31e5J"
      },
      "source": [
        "max_len = __\n",
        "train_dataset = SADataset(\n",
        "    \"\"\"Add arguments here\"\"\"\n",
        "\n",
        ")\n",
        "\n",
        "test_dataset = SADataset(\n",
        "    \"\"\"Add arguments here\"\"\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f0Tr_Jr4eAz"
      },
      "source": [
        "# Setup the HuggingFace trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the `bert-base-arabertv02` from HuggingFace models by Antoun et Al (2020). We can choose other Arabic BERT models by just changing the path here from `https://huggingface.co/models`."
      ],
      "metadata": {
        "id": "vi_EhH9CqH_N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVUtKhfwiyMZ"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now set up the training arguments, you can more information from https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"
      ],
      "metadata": {
        "id": "UbLm8trS2NTg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BtOuUXX4JD0"
      },
      "source": [
        "from transformers import Trainer , TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= \"./train\",\n",
        "    adam_epsilon = 1e-8,\n",
        "    learning_rate = 5e-5,\n",
        "    fp16 = True,\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size = 16,\n",
        "    gradient_accumulation_steps = 2,\n",
        "    num_train_epochs= 4,\n",
        "    do_eval = True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'eval_macro_f1',\n",
        "    greater_is_better = True,\n",
        "    seed = 42\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ0Kxrs46QI9"
      },
      "source": [
        "training_args.__dict__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Initialize the Trainer and start training:"
      ],
      "metadata": {
        "id": "C7UNtb2J3b8F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxuG93Aj5iiA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvxxCSpI-yGG"
      },
      "source": [
        "#  Saving the best model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before saving the model, let's change the label2id in the config file, and get the id to label map."
      ],
      "metadata": {
        "id": "qma6qGK1-6rj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPCX0NUt51wS"
      },
      "source": [
        "trainer.model.config.label2id = label_map\n",
        "inv_label_map = { v:k for k, v in label_map.items()}\n",
        "trainer.model.config.id2label = inv_label_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1UFwVNs-6Dk"
      },
      "source": [
        "#save the model in the folder\n",
        "trainer.save_model(\"best_sa_model\")\n",
        "test_dataset.tokenizer.save_pretrained(\"best_sa_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422JyvSi_n0d"
      },
      "source": [
        "# Loading the model for inference\n",
        "We can use HuggingFace pipelines to load the model for inference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzTSG6cp_g36"
      },
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model = \"best_sa_model\",\n",
        "        device=0, # set device to 0 for CUDA\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aJegKHL_zjk"
      },
      "source": [
        "pipe(\"انا لا احبك\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"انا احبك\")"
      ],
      "metadata": {
        "id": "5Sl3tW71-v92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}